{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1320530",
   "metadata": {},
   "source": [
    "# LLM Text Preprocessing Foundations (Embeddings)\n",
    "\n",
    "## Digital Transformation and Enterprise Solutions (TDSE)\n",
    "\n",
    "Este laboratorio explora los fundamentos del preprocesamiento textual para Large Language Models (LLMs), siguiendo el Chapter 2 del libro *Build a Large Language Model (From Scratch)* de Sebastian Raschka.\n",
    "\n",
    "El objetivo es comprender cómo el texto crudo se transforma en representaciones numéricas (embeddings) y por qué estas representaciones permiten a los modelos capturar significado, contexto y relaciones semánticas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el texto fuente\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:500])\n",
    "print(\"\\nTotal de caracteres:\", len(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc3a0a",
   "metadata": {},
   "source": [
    "### ¿Por qué este paso es importante para los LLMs?\n",
    "\n",
    "Los modelos de lenguaje no entienden texto directamente. Antes de cualquier aprendizaje, el texto existe únicamente como una secuencia de caracteres.  \n",
    "Este paso permite inspeccionar el estado más crudo de los datos y entender por qué es necesario transformarlos en estructuras más manejables para una red neuronal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d3334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Tokenización básica usando expresiones regulares\n",
    "tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "print(\"Primeros 50 tokens:\")\n",
    "print(tokens[:50])\n",
    "print(\"\\nNúmero total de tokens:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288daf6",
   "metadata": {},
   "source": [
    "### Importancia de la tokenización\n",
    "\n",
    "La tokenización define la unidad mínima que el modelo procesará.  \n",
    "Sin tokens:\n",
    "- No hay vocabulario\n",
    "- No existen embeddings\n",
    "- No puede haber aprendizaje\n",
    "\n",
    "En sistemas agenticos, una tokenización consistente es crucial para que diferentes componentes interpreten el lenguaje de forma compatible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción del vocabulario\n",
    "vocab = sorted(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for token, idx in token_to_id.items()}\n",
    "\n",
    "print(\"Tamaño del vocabulario:\", vocab_size)\n",
    "print(\"Ejemplo de mapeo token -> id:\")\n",
    "list(token_to_id.items())[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbfee1",
   "metadata": {},
   "source": [
    "### ¿Por qué crear un vocabulario?\n",
    "\n",
    "Las redes neuronales solo pueden operar con números.  \n",
    "El vocabulario permite mapear símbolos lingüísticos a identificadores numéricos, sirviendo como puente entre el lenguaje humano y el álgebra lineal utilizada por los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf468a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(token_list):\n",
    "    return [token_to_id[token] for token in token_list]\n",
    "\n",
    "def decode(id_list):\n",
    "    return [id_to_token[idx] for idx in id_list]\n",
    "\n",
    "encoded_tokens = encode(tokens)\n",
    "\n",
    "print(\"Tokens codificados (primeros 20):\")\n",
    "print(encoded_tokens[:20])\n",
    "\n",
    "print(\"\\nDecodificación de prueba:\")\n",
    "print(decode(encoded_tokens[:20]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7ef8d",
   "metadata": {},
   "source": [
    "### Codificación y decodificación\n",
    "\n",
    "Este proceso muestra que un LLM es, en esencia, un traductor entre símbolos y números.  \n",
    "La inteligencia no surge aquí, sino cuando estos números se transforman en vectores entrenables mediante embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Tokenizador GPT-2 basado en Byte Pair Encoding\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "bpe_tokens = tokenizer.encode(text)\n",
    "\n",
    "print(\"Primeros 50 tokens BPE:\")\n",
    "print(bpe_tokens[:50])\n",
    "print(\"\\nNúmero total de tokens BPE:\", len(bpe_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0771231",
   "metadata": {},
   "source": [
    "### ¿Por qué usar BPE en LLMs modernos?\n",
    "\n",
    "La tokenización BPE permite:\n",
    "- Manejar palabras raras o desconocidas\n",
    "- Reducir el tamaño del vocabulario\n",
    "- Mejorar la generalización\n",
    "- Trabajar a nivel sub-palabra\n",
    "\n",
    "Por esta razón es ampliamente usada en modelos como GPT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05cd1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(token_ids, max_length, stride):\n",
    "    samples = []\n",
    "    for i in range(0, len(token_ids) - max_length + 1, stride):\n",
    "        samples.append(token_ids[i:i + max_length])\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d55dd59",
   "metadata": {},
   "source": [
    "### Ventanas de contexto en LLMs\n",
    "\n",
    "Los LLMs procesan texto en bloques de tamaño fijo llamados ventanas de contexto.  \n",
    "Estas ventanas determinan cuánta información puede usar el modelo para predecir el siguiente token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bcc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento sin solapamiento\n",
    "max_length = 8\n",
    "stride = 8\n",
    "\n",
    "samples_no_overlap = create_samples(bpe_tokens, max_length, stride)\n",
    "print(\"Número de samples sin solapamiento:\", len(samples_no_overlap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ad86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento con solapamiento\n",
    "max_length = 8\n",
    "stride = 4\n",
    "\n",
    "samples_overlap = create_samples(bpe_tokens, max_length, stride)\n",
    "print(\"Número de samples con solapamiento:\", len(samples_overlap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65dd62",
   "metadata": {},
   "source": [
    "### Experimento: efecto de max_length y stride\n",
    "\n",
    "Al reducir el stride se incrementa el solapamiento entre ventanas, lo que:\n",
    "- Preserva continuidad semántica\n",
    "- Aumenta el número de muestras de entrenamiento\n",
    "- Mejora el aprendizaje contextual\n",
    "\n",
    "Este trade-off es fundamental en el entrenamiento de LLMs reales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973082e4",
   "metadata": {},
   "source": [
    "### Why do embeddings encode meaning, and how are they related to neural networks?\n",
    "\n",
    "Los embeddings son matrices entrenables que transforman tokens discretos en vectores densos continuos.  \n",
    "Durante el entrenamiento, el backpropagation ajusta estos vectores para que tokens usados en contextos similares queden cercanos en el espacio vectorial.\n",
    "\n",
    "Desde la perspectiva de redes neuronales:\n",
    "- Un embedding es una lookup table diferenciable\n",
    "- Es equivalente a una capa lineal sin bias\n",
    "- El significado emerge como geometría en el espacio vectorial\n",
    "\n",
    "El lenguaje no se programa: se aprende mediante optimización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3be2cf",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- El preprocesamiento textual es la base de cualquier LLM\n",
    "- La tokenización define el lenguaje interno del modelo\n",
    "- Las ventanas de contexto limitan y estructuran el aprendizaje\n",
    "- El solapamiento mejora la continuidad semántica\n",
    "- Los embeddings conectan lenguaje y álgebra lineal\n",
    "\n",
    "Este laboratorio demuestra cómo decisiones aparentemente simples tienen un impacto profundo en sistemas de IA modernos y soluciones empresariales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6cdbb",
   "metadata": {},
   "source": [
    "**Curso:** Digital Transformation and Enterprise Solutions (TDSE)  \n",
    "**Laboratorio:** LLM Text Preprocessing Foundations (Embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
